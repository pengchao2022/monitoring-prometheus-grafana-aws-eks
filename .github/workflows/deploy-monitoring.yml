name: Deploy Monitoring Stack to EKS

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  GRAFANA_ADMIN_USER: ${{ secrets.GRAFANA_ADMIN_USER }}
  GRAFANA_ADMIN_PASSWORD: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}

permissions:
  id-token: write
  contents: read

jobs:
  Deploy-Prometheus-Grafana-Helm:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Set up kubectl and helm
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

    - name: Clean up existing resources
      run: |
        helm uninstall kube-prometheus-stack -n monitoring --ignore-not-found=true
        kubectl delete job kube-prometheus-stack-admission-patch -n monitoring --ignore-not-found=true
        kubectl delete secret kube-prometheus-stack-grafana -n monitoring --ignore-not-found=true
        kubectl delete pvc kube-prometheus-stack-grafana -n monitoring --ignore-not-found=true

    - name: Create monitoring namespace
      run: |
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

    - name: Create Grafana Secret manually
      run: |
        # å…ˆæ‰‹åŠ¨åˆ›å»ºåŒ…å«æ­£ç¡®å‡­æ®çš„ Secret
        kubectl create secret generic kube-prometheus-stack-grafana \
          -n monitoring \
          --from-literal=admin-user="$GRAFANA_ADMIN_USER" \
          --from-literal=admin-password="$GRAFANA_ADMIN_PASSWORD" \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Create missing ServiceAccount
      run: |
        kubectl apply -f kubernetes/serviceaccount/admission-serviceaccount.yaml -n monitoring

    - name: Add prometheus-community helm repository
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update

    - name: Install kube-prometheus-stack with custom values
      run: |
        # å®‰è£…æ—¶å‘Šè¯‰ Helm ä½¿ç”¨ç°æœ‰çš„ Secret
        if [ -f "charts/kube-prometheus-stack/values-alb.yaml" ]; then
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --create-namespace \
            -f charts/kube-prometheus-stack/values-alb.yaml \
            --set nodeExporter.enabled=false \
            --set grafana.admin.user="" \
            --set grafana.admin.password="" \
            --set grafana.admin.existingSecret="kube-prometheus-stack-grafana"
        else
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --create-namespace \
            --set nodeExporter.enabled=false \
            --set grafana.admin.user="" \
            --set grafana.admin.password="" \
            --set grafana.admin.existingSecret="kube-prometheus-stack-grafana"
        fi

    - name: Verify installation and credentials
      run: |
        echo "=== ç­‰å¾… Grafana å¯åŠ¨ ==="
        sleep 60
        
        echo "=== æ£€æŸ¥ Grafana Pod çŠ¶æ€ ==="
        kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana
        
        echo "=== éªŒè¯ Secret å†…å®¹ ==="
        kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.data.admin-user}' | base64 --decode && echo
        kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.data.admin-password}' | base64 --decode && echo
        
        echo "=== æ£€æŸ¥ Grafana ç¯å¢ƒå˜é‡ ==="
        kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].spec.containers[0].env}' | jq '.[] | select(.name | startswith("GF_SECURITY"))'

    - name: Clean up existing Node Exporter
      run: |
        kubectl delete daemonset kube-prometheus-stack-prometheus-node-exporter -n monitoring --ignore-not-found=true
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter --ignore-not-found=true
        kubectl delete daemonset node-exporter-limited -n monitoring --ignore-not-found=true

    - name: Deploy Node Exporter ServiceAccount and RBAC
      run: |
        kubectl apply -f kubernetes/serviceaccount/node-exporter-serviceaccount.yaml -n monitoring

    - name: Deploy custom Node Exporter
      run: |
        kubectl apply -f kubernetes/node-exporter/daemonset-limited.yaml -n monitoring

    - name: Wait for Node Exporter to be ready
      run: |
        sleep 30
        echo "Checking Node Exporter status..."
        kubectl get daemonset node-exporter-limited -n monitoring
        kubectl get pods -n monitoring -l app.kubernetes.io/name=node-exporter -o wide

    - name: Deploy additional Kubernetes manifests
      run: |
        if [ -f "kubernetes/network-policies/monitoring-network-policy.yaml" ]; then
          kubectl apply -f kubernetes/network-policies/monitoring-network-policy.yaml -n monitoring || true
        fi
        kubectl apply -f kubernetes/ingress/ -n monitoring

    - name: Wait for ALB and get address
      id: get_elb
      run: |
        sleep 60
        ELB_ADDRESS=$(kubectl get ingress -n monitoring -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
        if [ -z "$ELB_ADDRESS" ]; then
          sleep 30
          ELB_ADDRESS=$(kubectl get ingress -n monitoring -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
        fi
        echo "ELB_ADDRESS=$ELB_ADDRESS" >> $GITHUB_OUTPUT

    - name: Configure Grafana for subpath support
      run: |
        ELB_ADDRESS="${{ steps.get_elb.outputs.ELB_ADDRESS }}"
        if [ -n "$ELB_ADDRESS" ]; then
          kubectl patch configmap kube-prometheus-stack-grafana -n monitoring --type=merge \
            --patch '{"data":{"grafana.ini":"[server]\ndomain = '"$ELB_ADDRESS"'\nroot_url = http://'"$ELB_ADDRESS"'/grafana/\nserve_from_sub_path = true\n"}}'
          kubectl rollout restart deployment kube-prometheus-stack-grafana -n monitoring
          sleep 60
        fi

    - name: Update Prometheus and Alertmanager external URLs
      run: |
        ELB_ADDRESS="${{ steps.get_elb.outputs.ELB_ADDRESS }}"
        if [ -n "$ELB_ADDRESS" ]; then
          kubectl get prometheus kube-prometheus-stack-prometheus -n monitoring && \
          kubectl patch prometheus kube-prometheus-stack-prometheus -n monitoring --type='merge' \
            -p='{"spec":{"externalUrl":"http://'$ELB_ADDRESS'/prometheus","routePrefix":"/prometheus"}}' || true
          kubectl get alertmanager kube-prometheus-stack-alertmanager -n monitoring && \
          kubectl patch alertmanager kube-prometheus-stack-alertmanager -n monitoring --type='merge' \
            -p='{"spec":{"externalUrl":"http://'$ELB_ADDRESS'/alertmanager","routePrefix":"/alertmanager"}}' || true
        fi

    - name: Restart monitoring services
      run: |
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=prometheus --ignore-not-found=true || true
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=alertmanager --ignore-not-found=true || true
        sleep 30

    - name: Output access information
      run: |
        echo "=========================================="
        echo "ğŸš€ Deployment Finished"
        echo "=========================================="
        echo ""
        echo "EKS Cluster: ${{ env.CLUSTER_NAME }}"
        echo "AWS Region: ${{ env.AWS_REGION }}"
        echo ""
        echo "ğŸ“Š Prometheus: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/prometheus"
        echo "ğŸ“ˆ Grafana: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/grafana"
        echo "ğŸš¨ Alertmanager: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/alertmanager"
        echo ""
        echo "Grafanaç™»å½•å‡­æ®:"
        echo "ç”¨æˆ·å: $GRAFANA_ADMIN_USER"
        echo "å¯†ç : $GRAFANA_ADMIN_PASSWORD"
        echo ""
        echo "å¦‚æœä»ç„¶æ— æ³•ç™»å½•ï¼Œè¯·æ£€æŸ¥:"
        echo "1. å¯†ç æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦"
        echo "2. å¯ä»¥å°è¯•é‡ç½®å¯†ç :"
        echo "   kubectl delete secret kube-prometheus-stack-grafana -n monitoring"
        echo "   kubectl create secret generic kube-prometheus-stack-grafana -n monitoring --from-literal=admin-user='$GRAFANA_ADMIN_USER' --from-literal=admin-password='$GRAFANA_ADMIN_PASSWORD'"
        echo "   kubectl rollout restart deployment kube-prometheus-stack-grafana -n monitoring"
        echo ""
        echo "Node Exporter Status:"
        kubectl get pods -n monitoring -l app.kubernetes.io/name=node-exporter