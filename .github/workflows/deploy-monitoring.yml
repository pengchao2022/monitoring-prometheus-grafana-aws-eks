name: Deploy Monitoring Stack to EKS

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}

permissions:
  id-token: write
  contents: read

jobs:
  Deploy-Prometheus-Grafana-Helm:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Set up kubectl and helm
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

    - name: Clean up existing resources
      run: |
        helm uninstall kube-prometheus-stack -n monitoring --ignore-not-found=true
        kubectl delete job kube-prometheus-stack-admission-patch -n monitoring --ignore-not-found=true
        kubectl delete secret kube-prometheus-stack-grafana -n monitoring --ignore-not-found=true
        kubectl delete pvc kube-prometheus-stack-grafana -n monitoring --ignore-not-found=true

    - name: Create monitoring namespace
      run: |
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

    - name: Create missing ServiceAccount
      run: |
        kubectl apply -f kubernetes/serviceaccount/admission-serviceaccount.yaml -n monitoring

    - name: Add prometheus-community helm repository
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update

    - name: Install kube-prometheus-stack with custom values
      run: |
        # 先安装但不设置密码，让系统生成默认的
        if [ -f "charts/kube-prometheus-stack/values-alb.yaml" ]; then
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --create-namespace \
            -f charts/kube-prometheus-stack/values-alb.yaml \
            --set nodeExporter.enabled=false
        else
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --create-namespace \
            --set nodeExporter.enabled=false
        fi

    - name: Force reset Grafana credentials
      run: |
        # 等待 Grafana 启动
        sleep 30
        
        # 删除现有的 Secret
        kubectl delete secret kube-prometheus-stack-grafana -n monitoring --ignore-not-found=true
        
        # 使用 kubectl 手动创建包含正确凭据的 Secret
        kubectl create secret generic kube-prometheus-stack-grafana \
          -n monitoring \
          --from-literal=admin-user="${{ secrets.GRAFANA_ADMIN_USER }}" \
          --from-literal=admin-password="${{ secrets.GRAFANA_ADMIN_PASSWORD }}" \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # 重启 Grafana 以使用新凭据
        kubectl rollout restart deployment kube-prometheus-stack-grafana -n monitoring
        sleep 30

    - name: Verify Grafana credentials
      run: |
        echo "=== 验证 Grafana 凭据 ==="
        echo "Secret 中的用户名:"
        kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.data.admin-user}' | base64 --decode && echo
        echo "Secret 中的密码:"
        kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.data.admin-password}' | base64 --decode && echo
        echo "GitHub Secrets 中的用户名: ${{ secrets.GRAFANA_ADMIN_USER }}"
        echo "GitHub Secrets 中的密码长度: ${#{{ secrets.GRAFANA_ADMIN_PASSWORD }}} 字符"

    - name: Clean up existing Node Exporter
      run: |
        kubectl delete daemonset kube-prometheus-stack-prometheus-node-exporter -n monitoring --ignore-not-found=true
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter --ignore-not-found=true
        kubectl delete daemonset node-exporter-limited -n monitoring --ignore-not-found=true

    - name: Deploy Node Exporter ServiceAccount and RBAC
      run: |
        kubectl apply -f kubernetes/serviceaccount/node-exporter-serviceaccount.yaml -n monitoring

    - name: Deploy custom Node Exporter
      run: |
        kubectl apply -f kubernetes/node-exporter/daemonset-limited.yaml -n monitoring

    - name: Wait for Node Exporter to be ready
      run: |
        sleep 30
        echo "Checking Node Exporter status..."
        kubectl get daemonset node-exporter-limited -n monitoring
        kubectl get pods -n monitoring -l app.kubernetes.io/name=node-exporter -o wide

    - name: Deploy additional Kubernetes manifests
      run: |
        if [ -f "kubernetes/network-policies/monitoring-network-policy.yaml" ]; then
          kubectl apply -f kubernetes/network-policies/monitoring-network-policy.yaml -n monitoring || true
        fi
        kubectl apply -f kubernetes/ingress/ -n monitoring

    - name: Wait for ALB and get address
      id: get_elb
      run: |
        sleep 60
        ELB_ADDRESS=$(kubectl get ingress -n monitoring -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
        if [ -z "$ELB_ADDRESS" ]; then
          sleep 30
          ELB_ADDRESS=$(kubectl get ingress -n monitoring -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
        fi
        echo "ELB_ADDRESS=$ELB_ADDRESS" >> $GITHUB_OUTPUT

    - name: Configure Grafana for subpath support
      run: |
        ELB_ADDRESS="${{ steps.get_elb.outputs.ELB_ADDRESS }}"
        if [ -n "$ELB_ADDRESS" ]; then
          kubectl patch configmap kube-prometheus-stack-grafana -n monitoring --type=merge \
            --patch '{"data":{"grafana.ini":"[server]\ndomain = '"$ELB_ADDRESS"'\nroot_url = http://'"$ELB_ADDRESS"'/grafana/\nserve_from_sub_path = true\n"}}'
          kubectl rollout restart deployment kube-prometheus-stack-grafana -n monitoring
          sleep 60
        fi

    - name: Update Prometheus and Alertmanager external URLs
      run: |
        ELB_ADDRESS="${{ steps.get_elb.outputs.ELB_ADDRESS }}"
        if [ -n "$ELB_ADDRESS" ]; then
          kubectl get prometheus kube-prometheus-stack-prometheus -n monitoring && \
          kubectl patch prometheus kube-prometheus-stack-prometheus -n monitoring --type='merge' \
            -p='{"spec":{"externalUrl":"http://'$ELB_ADDRESS'/prometheus","routePrefix":"/prometheus"}}' || true
          kubectl get alertmanager kube-prometheus-stack-alertmanager -n monitoring && \
          kubectl patch alertmanager kube-prometheus-stack-alertmanager -n monitoring --type='merge' \
            -p='{"spec":{"externalUrl":"http://'$ELB_ADDRESS'/alertmanager","routePrefix":"/alertmanager"}}' || true
        fi

    - name: Restart monitoring services
      run: |
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=prometheus --ignore-not-found=true || true
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=alertmanager --ignore-not-found=true || true
        sleep 30

    - name: Output access information
      run: |
        echo "=========================================="
        echo "🚀 Deployment Finished"
        echo "=========================================="
        echo ""
        echo "EKS Cluster: ${{ env.CLUSTER_NAME }}"
        echo "AWS Region: ${{ env.AWS_REGION }}"
        echo ""
        echo "📊 Prometheus: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/prometheus"
        echo "📈 Grafana: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/grafana"
        echo "🚨 Alertmanager: http://${{ steps.get_elb.outputs.ELB_ADDRESS }}/alertmanager"
        echo ""
        echo "Grafana登录凭据:"
        echo "用户名: ${{ secrets.GRAFANA_ADMIN_USER }}"
        echo "密码: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}"
        echo ""
        echo "如果仍然无法登录，请检查:"
        echo "1. GitHub Secrets 中的 GRAFANA_ADMIN_USER 和 GRAFANA_ADMIN_PASSWORD 是否正确设置"
        echo "2. 密码是否包含特殊字符"
        echo "3. 可以尝试使用以下命令手动验证:"
        echo "   kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.data.admin-password}' | base64 --decode"
        echo ""
        echo "Node Exporter Status:"
        kubectl get pods -n monitoring -l app.kubernetes.io/name=node-exporter