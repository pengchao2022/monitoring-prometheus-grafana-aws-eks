name: Deploy Monitoring Stack with ALB

on:
  push:
    branches: [ main, master ]
    paths:
      - 'charts/**'
      - 'kubernetes/**'
      - '.github/workflows/deploy-monitoring.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'éƒ¨ç½²ç¯å¢ƒ'
        required: true
        default: 'production'
        type: choice
        options:
        - staging
        - production

env:
  CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
  AWS_REGION: ${{ secrets.AWS_REGION }}
  NAMESPACE: monitoring

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig \
          --region ${{ env.AWS_REGION }} \
          --name ${{ env.CLUSTER_NAME }}

    - name: Setup kubectl and Helm
      run: |
        # å®‰è£… kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
        # å®‰è£… Helm
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
        
        # éªŒè¯å®‰è£…
        kubectl version --client
        helm version

    - name: Force cleanup stuck Helm operations
      run: |
        echo "ğŸ§¹ FORCE CLEANUP: Removing all stuck Helm operations..."
        
        # æ–¹æ³•1: æ£€æŸ¥å¹¶åˆ é™¤æ‰€æœ‰è¿›è¡Œä¸­çš„ Helm secret
        echo "=== Deleting all Helm secrets with pending status ==="
        kubectl get secrets -n monitoring -l owner=helm -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' 2>/dev/null | while read secret; do
          status=$(kubectl get secret "$secret" -n monitoring -o jsonpath='{.metadata.labels.status}' 2>/dev/null || echo "unknown")
          echo "Secret: $secret (status: $status)"
          if [ "$status" = "pending" ] || [ "$status" = "uninstalling" ] || [ "$status" = "superseded" ]; then
            echo "ğŸš¨ DELETING stuck secret: $secret"
            kubectl delete secret "$secret" -n monitoring --ignore-not-found=true
          fi
        done
        
        # æ–¹æ³•2: å¼ºåˆ¶åˆ é™¤æ‰€æœ‰ Helm ç›¸å…³ secretï¼ˆæ›´æ¿€è¿›ï¼‰
        echo "=== Force deleting ALL Helm secrets in monitoring namespace ==="
        kubectl delete secrets -n monitoring -l owner=helm --ignore-not-found=true
        
        # æ–¹æ³•3: æ£€æŸ¥ release çŠ¶æ€å¹¶å¼ºåˆ¶æ¸…ç†
        echo "=== Checking Helm release status ==="
        if helm status kube-prometheus-stack -n monitoring &>/dev/null; then
          echo "Release exists, getting detailed status..."
          helm status kube-prometheus-stack -n monitoring
          
          echo "ğŸš¨ FORCE DELETING the release..."
          helm uninstall kube-prometheus-stack -n monitoring --wait --timeout 5m 2>/dev/null || \
          helm delete kube-prometheus-stack -n monitoring 2>/dev/null || \
          echo "Release deletion attempted"
        else
          echo "No existing release found"
        fi
        
        # æ–¹æ³•4: æ¸…ç†ä»»ä½•æ®‹ç•™çš„é…ç½®
        echo "=== Cleaning up any remaining resources ==="
        kubectl delete daemonset -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter --ignore-not-found=true
        kubectl delete pods -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter --ignore-not-found=true
        kubectl delete deployment -n monitoring -l app=kube-prometheus-stack-operator --ignore-not-found=true
        
        # ç­‰å¾…ç¡®ä¿æ¸…ç†å®Œæˆ
        sleep 10
        echo "âœ… Force cleanup completed"

    - name: Pre-deployment cluster check
      run: |
        echo "ğŸ” Pre-deployment Cluster Diagnostics"
        echo "====================================="
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        echo "=== Node Status ==="
        kubectl get nodes -o wide
        
        # æ£€æŸ¥èŠ‚ç‚¹èµ„æº
        echo "=== Node Resources ==="
        kubectl top nodes 2>/dev/null || echo "Metrics server not available"
        
        # æ£€æŸ¥å­˜å‚¨ç±»
        echo "=== Storage Classes ==="
        kubectl get storageclass
        
        # æ£€æŸ¥ç°æœ‰å‘½åç©ºé—´
        echo "=== Existing Monitoring Namespace ==="
        kubectl get namespace ${{ env.NAMESPACE }} || echo "Namespace does not exist yet"

    - name: Verify AWS Load Balancer Controller
      run: |
        echo "ğŸ” Checking existing AWS Load Balancer Controller..."
        kubectl get pods -A | grep aws-load-balancer-controller || echo "âŒ AWS Load Balancer Controller not found"

    - name: Create monitoring namespace
      run: |
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

    - name: Create Grafana secret
      run: |
        kubectl create secret generic grafana-admin-secret \
          --namespace=${{ env.NAMESPACE }} \
          --from-literal=admin-user=${{ secrets.GRAFANA_ADMIN_USER }} \
          --from-literal=admin-password=${{ secrets.GRAFANA_ADMIN_PASSWORD }} \
          --dry-run=client -o yaml | kubectl apply -f -

    - name: Cleanup existing node-exporters
      run: |
        echo "ğŸ§¹ Cleaning up existing node-exporters..."
        
        # åˆ é™¤é»˜è®¤çš„ node-exporter DaemonSet
        kubectl delete daemonset kube-prometheus-stack-prometheus-node-exporter -n monitoring --ignore-not-found=true
        
        # åˆ é™¤æ‰€æœ‰ node-exporter pods
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter --ignore-not-found=true
        
        # åˆ é™¤è‡ªå®šä¹‰çš„ node-exporterï¼ˆå¦‚æœæœ‰ï¼‰
        kubectl delete daemonset node-exporter-limited -n monitoring --ignore-not-found=true
        kubectl delete pod -n monitoring -l app.kubernetes.io/name=node-exporter --ignore-not-found=true
        
        echo "âœ… Cleanup completed"

    - name: Add Helm repositories
      run: |
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        helm repo update

    - name: Deploy kube-prometheus-stack with timeout
      id: helm-deploy
      run: |
        echo "ğŸš€ Starting kube-prometheus-stack installation..."
        echo "Start time: $(date)"
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›è¡Œä¸­çš„æ“ä½œ
        echo "=== Checking for any remaining stuck operations ==="
        kubectl get secrets -n monitoring -l owner=helm -o jsonpath='{range .items[*]}{.metadata.name}{" "}{.metadata.labels.status}{"\n"}{end}' 2>/dev/null || echo "No Helm secrets found"
        
        # è®¾ç½®å®‰è£…è¶…æ—¶å’Œé‡è¯•é€»è¾‘
        max_attempts=3
        attempt=1
        success=false
        
        while [ $attempt -le $max_attempts ]; do
          echo "Attempt $attempt of $max_attempts..."
          
          if helm upgrade --install kube-prometheus-stack \
            prometheus-community/kube-prometheus-stack \
            --namespace ${{ env.NAMESPACE }} \
            --version 48.1.1 \
            --values charts/kube-prometheus-stack/values-alb.yaml \
            --wait \
            --timeout 15m \
            --atomic \
            --create-namespace; then
            
            echo "âœ… Helm installation successful on attempt $attempt"
            success=true
            break
          else
            echo "âŒ Helm installation failed on attempt $attempt"
            if [ $attempt -lt $max_attempts ]; then
              echo "Waiting 30 seconds before retry..."
              sleep 30
              # æ¯æ¬¡é‡è¯•å‰å†æ¬¡æ¸…ç†
              echo "Cleaning up before retry..."
              kubectl delete secrets -n monitoring -l owner=helm --ignore-not-found=true
            fi
          fi
          attempt=$((attempt + 1))
        done
        
        if [ "$success" = true ]; then
          echo "helm_status=success" >> $GITHUB_OUTPUT
        else
          echo "helm_status=failed" >> $GITHUB_OUTPUT
        fi
        
        echo "Completion time: $(date)"

    - name: Verify node-exporter deployment
      if: steps.helm-deploy.outputs.helm_status == 'success'
      run: |
        echo "ğŸ” Verifying node-exporter deployment..."
        
        echo "=== Node-exporter DaemonSet Status ==="
        kubectl get daemonset -n monitoring | grep node-exporter || echo "No node-exporter DaemonSet found"
        
        echo "=== Node-exporter Pods ==="
        kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter
        
        echo "=== Pods Distribution ==="
        kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter -o wide

    - name: Real-time installation monitoring
      if: always()
      run: |
        echo "ğŸ“Š Real-time Installation Monitoring"
        echo "==================================="
        
        # æŒç»­ç›‘æ§ Pod çŠ¶æ€
        for i in {1..6}; do
          echo "--- Status check $i --- $(date)"
          echo "Pods:"
          kubectl get pods -n ${{ env.NAMESPACE }} --sort-by=.status.startTime | head -10
          
          echo "Events:"
          kubectl get events -n ${{ env.NAMESPACE }} --sort-by=.lastTimestamp --field-selector type=Warning 2>/dev/null | tail -3 || echo "No warning events"
          
          if [ $i -lt 6 ]; then
            sleep 20
          fi
        done

    - name: Post-installation diagnostics
      if: always()
      run: |
        echo "ğŸ” Post-installation Diagnostics"
        echo "================================"
        
        # æ£€æŸ¥ Helm release çŠ¶æ€
        echo "=== Helm Release Status ==="
        helm list -n ${{ env.NAMESPACE }} || echo "Helm release not found"
        
        # è¯¦ç»† Pod è¯Šæ–­
        echo "=== Detailed Pod Diagnostics ==="
        kubectl get pods -n ${{ env.NAMESPACE }} -o wide
        
        # æ£€æŸ¥ node-exporter çŠ¶æ€
        echo "=== Node-Exporter Status ==="
        kubectl get daemonset -n ${{ env.NAMESPACE }} | grep node-exporter || echo "No node-exporter daemonset found"
        kubectl get pods -n ${{ env.NAMESPACE }} -l app.kubernetes.io/name=prometheus-node-exporter 2>/dev/null || echo "No node-exporter pods found"
        
        # æ£€æŸ¥ Pod è¯¦ç»†çŠ¶æ€ï¼ˆåªæ£€æŸ¥å‰3ä¸ªï¼‰
        echo "=== Pod Events (first 3) ==="
        for pod in $(kubectl get pods -n ${{ env.NAMESPACE }} --no-headers -o custom-columns=":metadata.name" | head -3); do
          echo "--- Pod: $pod ---"
          kubectl describe pod $pod -n ${{ env.NAMESPACE }} | grep -A 5 "Events:" || echo "No events found"
        done
        
        # æ£€æŸ¥ PVC çŠ¶æ€
        echo "=== PVC Status ==="
        kubectl get pvc -n ${{ env.NAMESPACE }} -o wide 2>/dev/null || echo "No PVCs found"
        
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        echo "=== Service Status ==="
        kubectl get services -n ${{ env.NAMESPACE }}
        
        # æ£€æŸ¥æœ€è¿‘çš„äº‹ä»¶
        echo "=== Recent Events (last 10) ==="
        kubectl get events -n ${{ env.NAMESPACE }} --sort-by=.lastTimestamp | tail -10

    - name: Deploy Ingress resources
      if: steps.helm-deploy.outputs.helm_status == 'success'
      run: |
        echo "ğŸš€ Deploying Ingress resources..."
        kubectl apply -f kubernetes/ingress/ -n ${{ env.NAMESPACE }}

    - name: Wait for ALB provisioning
      if: steps.helm-deploy.outputs.helm_status == 'success'
      run: |
        echo "â³ Waiting for ALB provisioning..."
        
        # ç­‰å¾… ALB åˆ›å»ºï¼Œå¸¦è¶…æ—¶
        max_wait=300
        wait_time=0
        alb_provisioned=false
        
        while [ $wait_time -lt $max_wait ]; do
          ALB_HOSTNAME=$(kubectl get ingress grafana-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
          if [ -n "$ALB_HOSTNAME" ]; then
            echo "âœ… ALB provisioned: $ALB_HOSTNAME"
            alb_provisioned=true
            break
          fi
          echo "Waiting for ALB... (elapsed: ${wait_time}s)"
          sleep 30
          wait_time=$((wait_time + 30))
        done
        
        if [ "$alb_provisioned" = false ]; then
          echo "âŒ ALB provisioning timeout after ${max_wait}s"
        fi
        
        echo "ALB Status:"
        kubectl get ingress -n ${{ env.NAMESPACE }}

    - name: Final health check
      if: steps.helm-deploy.outputs.helm_status == 'success'
      run: |
        echo "ğŸ¥ Running final health check..."
        if [ -f "scripts/health-check.sh" ]; then
          chmod +x scripts/health-check.sh
          ./scripts/health-check.sh
        else
          echo "â„¹ï¸ No health-check script found, skipping"
        fi

    - name: Get ALB endpoints
      if: steps.helm-deploy.outputs.helm_status == 'success'
      id: endpoints
      run: |
        GRAFANA_ALB=$(kubectl get ingress grafana-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
        PROMETHEUS_ALB=$(kubectl get ingress prometheus-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
        ALERTMANAGER_ALB=$(kubectl get ingress alertmanager-ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
        
        echo "grafana_alb=$GRAFANA_ALB" >> $GITHUB_OUTPUT
        echo "prometheus_alb=$PROMETHEUS_ALB" >> $GITHUB_OUTPUT
        echo "alertmanager_alb=$ALERTMANAGER_ALB" >> $GITHUB_OUTPUT
        
        echo "Grafana ALB: $GRAFANA_ALB"
        echo "Prometheus ALB: $PROMETHEUS_ALB"
        echo "Alertmanager ALB: $ALERTMANAGER_ALB"

    - name: Create deployment summary
      if: always()
      run: |
        # ä½¿ç”¨ç®€å•çš„ echo è¾“å‡ºéƒ¨ç½²ç»“æœï¼Œé¿å… github-script çš„é—®é¢˜
        HELM_STATUS="${{ steps.helm-deploy.outputs.helm_status }}"
        GRAFANA_ALB="${{ steps.endpoints.outputs.grafana_alb }}"
        PROMETHEUS_ALB="${{ steps.endpoints.outputs.prometheus_alb }}"
        ALERTMANAGER_ALB="${{ steps.endpoints.outputs.alertmanager_alb }}"
        
        echo "=========================================="
        echo "         DEPLOYMENT SUMMARY"
        echo "=========================================="
        
        if [ "$HELM_STATUS" = "success" ]; then
          echo "ğŸ‰ Monitoring Stack with ALB éƒ¨ç½²å®Œæˆ!"
          echo ""
          echo "ALB è®¿é—®åœ°å€:"
          echo "ğŸ“Š Grafana: http://$GRAFANA_ALB"
          echo "ğŸ“ˆ Prometheus: http://$PROMETHEUS_ALB" 
          echo "ğŸš¨ Alertmanager: http://$ALERTMANAGER_ALB"
          echo ""
          echo "ç™»å½•ä¿¡æ¯:"
          echo "Grafana ç”¨æˆ·å: ${{ secrets.GRAFANA_ADMIN_USER }}"
          echo "Grafana å¯†ç : [å·²è®¾ç½®]"
          echo ""
          echo "ç»„ä»¶çŠ¶æ€:"
          echo "- Grafana: âœ… è¿è¡Œä¸­"
          echo "- Prometheus: âœ… è¿è¡Œä¸­" 
          echo "- Alertmanager: âœ… è¿è¡Œä¸­"
          echo "- Node-Exporter: âœ… åœ¨2ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œ"
          echo ""
          echo "ä¸‹ä¸€æ­¥:"
          echo "1. ç­‰å¾… ALB DNS ä¼ æ’­ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰"
          echo "2. åœ¨æµè§ˆå™¨ä¸­è®¿é—®ä¸Šè¿° URL"
          echo "3. æ ¹æ®éœ€è¦é…ç½®é¢å¤–çš„ä»ªè¡¨æ¿å’Œå‘Šè­¦"
        else
          echo "âŒ Monitoring Stack éƒ¨ç½²å¤±è´¥"
          echo ""
          echo "å¯èƒ½çš„åŸå› :"
          echo "1. Helm å®‰è£…è¶…æ—¶"
          echo "2. é›†ç¾¤èµ„æºä¸è¶³"
          echo "3. ç½‘ç»œé—®é¢˜å¯¼è‡´é•œåƒæ‹‰å–å¤±è´¥"
          echo "4. å­˜å‚¨ç±»é…ç½®é—®é¢˜"
          echo "5. Helm æ“ä½œè¢«å¡ä½ï¼ˆéœ€è¦æ‰‹åŠ¨æ¸…ç†ï¼‰"
          echo ""
          echo "è¯·æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—äº†è§£å…·ä½“é”™è¯¯ä¿¡æ¯ã€‚"
        fi
        echo "=========================================="

    - name: Upload deployment logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: deployment-logs
        path: |
          ${{ github.workspace }}/**/*.log
        retention-days: 7